# AE-and-VAE
In this lab focused on Autoencoder (AE) and Variational Autoencoder (VAE) architectures using the MNIST dataset, the participant:

Autoencoder (AE):

Developed an AE with an adjustable hidden layer configuration for image reconstruction.
Explored hyperparameters (learning rate, batch size, epochs, and hidden layers) to optimize performance.
Evaluated the AE using loss curves during training and assessed image reconstruction quality.
Variational Autoencoder (VAE):

Designed a VAE with an encoder-decoder structure and an adjustable latent space size.
Tuned hyperparameters (learning rate, batch size, epochs, and latent space size) for optimal results.
Evaluated the VAE through loss and Kullback-Leibler divergence plots, examining reconstruction quality and latent space impact.
Model Comparison and Analysis:

Compared AE and VAE performance, highlighting VAE advantages in capturing a meaningful latent space distribution.
Extracted insights from loss curves and relevant metrics, understanding training dynamics and convergence behavior.
Latent Space Visualization:

Plotted latent spaces for both AE and VAE, observing distributions and identifying patterns or clusters within the data.
In summary, the lab provided a comprehensive experience in designing, training, and evaluating AE and VAE models.The comparative analysis enhanced understanding of when to choose a VAE over a traditional AE.
